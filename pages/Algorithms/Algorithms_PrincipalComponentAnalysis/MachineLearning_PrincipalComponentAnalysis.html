<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Algorithms | Principal Component Analysis</title>

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="../../../bootstrap/css/bootstrap.min.css">

    <script src="../../../automation/loadObjects.js"></script>

    <script src="../../../external_libraries/bootstrap/jquery.min.js"></script>
    <script src="../../../external_libraries/bootstrap/popper.min.js"></script>
    <script src="../../../external_libraries/bootstrap/bootstrap.min.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script>

    <link rel="stylesheet" href="../../../js/highlight/styles/default.css">
    <link rel="stylesheet" href="../../../index.css">
    <script src="../../../js/highlight/highlight.pack.js"></script>

    <script src="../../../js/common.js"></script>
    <script src="../../../automation/loadObjects.js"></script>
    <script>loadHeaderFooter("../../../automation/header.html", "../../../automation/footer.html")</script>
    <script>hljs.initHighlightingOnLoad();</script>


    <style>

        #slideshow-container {
            position: relative;
            max-width: 800px;
            margin: auto;
        }

        .slide {
            display: none;
            width: 100%;
        }

        .active {
            display: block;
        }

        .slideshow-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 2rem;
            justify-items: center;
            align-items: center;
        }

        .slideshow-grid > div {
            background: none !important;
            border: none !important;
            border-radius: 0 !important;
            padding: 0 !important;
        }

        /* Responsive code block for mobile */
        pre code {
            white-space: pre-wrap;
            word-break: break-word;
            display: block;
            overflow-x: auto;
            max-width: 100vw;
            box-sizing: border-box;
        }

        pre {
            overflow-x: auto;
            max-width: 100vw;
            box-sizing: border-box;
        }


    </style>

</head>

<body class="container-lg card p-5">

<div id="header_placeholder"></div>

<div class="jumbotron-fluid text-center p-2">
    <h1>Algorithms | Principal Component Analysis</h1>
</div>


<div class="container-md">

    <a href="Algorithms_PrincipalComponentAnalysis_HeaderImage.png" target="_blank"><img
            src="Algorithms_PrincipalComponentAnalysis_HeaderImage.png"
            class="container-lg centered_header_image"></a>

    <div class="">
        <br><h5>Scenario</h5>
        <ul>
            <li>Principal component analysis is a concept developed using techniques form Linear Algebra</li>
            <li>This technique is widely used in Statistics and Sciences to map the higher dimensional data into a lower
                dimensional data while retaining as much as variance for the respective features as possible.
            </li>
            <li>Dimensionality reduction makes usage of large data structures such as embeddings from foundation
                models. Let us explore the algorithm here.
            </li>
        </ul>

        <h5>Algorithm</h5>
        <ul>
            <li>Dimensions \((n_{features}, n_{samples}) \) | Start with the raw embeddings matrix that have to be
                projected called as \(X\)
            </li>
            <li>Dimensions \((n_{features}, n_{samples})\) | Normalize every feature in this dataset \(X_{Std} = \frac{X
                - \mu}{\sigma}\) where \(\mu\) is the per feature mean and \(\sigma\) is the per feature standard
                deviation.
            </li>
            <li>Dimensions \((n_{features}, n_{features})\) | Calculate covariance matrix of the above as \[Cov =
                \frac{X_{Std}^T \cdot X_{Std}}{n_{samples}-1}\]
            </li>
            <li>Dimensions \((n_{features}, n_{features})\) | Decompose the \(Cov\) matrix into its eigenvalues and
                eigenvectors as \(Cov = W \cdot \Lambda \cdot W^{-1}\) where
                <ul>
                    <li class="text-center list-unstyled">\(\Lambda\) = diagonal matrix of eigen values shown as
                        \((\lambda_1,\lambda_2,\lambda_3,...)\)
                    </li>
                    <li class="text-center list-unstyled">\(W\) = Matrix of eigenvectors (columns for principal
                        components)
                    </li>
                </ul>
            </li>
            <li>Dimensions \((n_{features}, n_{features})\) | Sort Eigen Vectors in the descending order as
                \((\lambda_{x1} \ge \lambda_{x2} \ge \lambda_{x3} \ge ...)\)
            </li>
            <li>Dimensions \((n_{features}, n_{k})\) | Select top \(k\) vectors. Where \(k\) is the dimension of
                the preferred projection space and \(W_k\) is the projection matrix.
            </li>
            <li>Dimensions \((n_{samples}, n_{k})\) | Multiply \(X_{std}\) with \(W_k\) to create the projected matrix
                \[X_{projected} = X_{std} \cdot W_k\]
            </li>
        </ul>

        <h5>Sources</h5>
        <p><strong>Original Vectors</strong> sourced from the model <em>deepseek-coder-1.3b-base</em></p>


        <pre><code class="language-python">
    model_name = "deepseek-ai/deepseek-coder-1.3b-base"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    print(f"DeepSeek-Coder model '{model_name}' and tokenizer loaded.")
        </code></pre>

        <p><strong></strong>Top 512 words of the english language have been encoded using the model above</p>


        <h5>How to determine the best candidate?</h5>
        <ul>
            <li>Calculate all combinations pairwise similarities for a subset of the vectors. This exercise samples 1024
                vectors
            </li>
            <li>Pick a ballpark output dimension size</li>
            <li>Experiment with the available algorithms and various output dimensions</li>
            <li>Calculate the similarities in the lower dimensional space</li>
            <li>Subtract the corresponding similarities pairs of higher and lower dimensions</li>
            <li>Plot the distributions</li>
            <li>Pick the method with a distribution centered normally with a mean closest to 0 and with the least
                variance
            </li>
        </ul>

        <h5>Keys</h5>
        <p>There are several abbreviations here and the guide below will help with the reading</p>
        <ul>
            <li>Original | 2048 \(\rightarrow\) Raw vectors from the <em>deepseek-coder-1.3b-base</em> model</li>
            <li>PCA | 0016\(\rightarrow\) Principal Component Analysis to hash vectors to dimensions 0016</li>
            <li>PCA | 0128\(\rightarrow\) Principal Component Analysis to hash vectors to dimensions 0128</li>
        </ul>

        <h5>Observations</h5>
        <p>From the distributions below, we can observe the following in the similarity space.</p>
        <ul>
            <li>We can see in the distribution charts that just like the Original vectors, the projected vectors are
                also exhibiting a similar skewness in the cosine similarity space.
            </li>
            <li>But the real usefulness of these vectors can be seen in the second distribution chart.</li>
            <li>As we see, the differences are getting smaller and their peaks are getting very close to 0.</li>
        </ul>

        <h5>Victors?</h5>
        <ul>
            <li>From the looks of it, any of the projections above the dimensions of 64 can be used as approximations
            </li>
            <li>All of them have been highlighted with a green box in the histogram charts</li>
            <li>The histogram outlined in red is the original distribution. The one we are not trying to use.</li>
        </ul>


        <hr>
        <h5>Distribution Charts</h5>

        <img class="centered_image_auto_width"
             src="MachineLearning_PrincipalComponentAnalysis_DistributionChart_Original.svg">
        <img class="centered_image_auto_width"
             src="MachineLearning_PrincipalComponentAnalysis_DistributionChart_Delta.svg">


        <hr>
        <h5 class="">1. Base Distributions in <em>Cosine Similarity Space</em></h5>
        <img class="centered_image_auto_width" src="MachineLearning_PrincipalComponentAnalysis_Histogram_Original.svg">

        <hr>
        <h5 class="">2. Difference distributions in <em>Cosine Similarity Space</em></h5>
        <p>These are obtained by subtracting pairwise cosine similarities between the original and reduced
            dimensions. For example, a given data point would be \(a_{2048} - a_{PCA\ 0064}\) where \(a_{2048}\) is the
            cosine similarity between a pair of words and \(a_{PCA\ 0064}\) is the cosine similarity between the exact
            same words but with the vectors obtained from the PCA projector having output dimensions of 64. Each
            distribution is made up of nearly 130,000 datapoints.
        </p>
        <img class="centered_image_auto_width" src="MachineLearning_PrincipalComponentAnalysis_Histogram_Delta.svg">

        <hr>
        <h5 class="">3. Base and Difference distributions in <em>Cosine Similarity Space</em></h5>
        <img class="centered_image_auto_width"
             src="MachineLearning_PrincipalComponentAnalysis_Histogram_OriginalAndDelta.svg">
        <br>

    </div>
</div>

<div class="container-md">
    <br><h5 class="">Full Code</h5>

    <div class="">
        <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
            <iframe

                    src="https://nbviewer.org/github/fermibot/python-projects/blob/main/machine_learning/MachineLearning_PrincipalComponentAnalysis/MachineLearning_PrincipalComponentAnalysis.ipynb"
                    style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border: none;"
                    loading="lazy"
            ></iframe>
        </div>
    </div>


</div>

<div class="p-5"></div>

<div id="footer_placeholder"></div>

</body>
</html>